#!/bin/sh
#################################################################
#  Copyright notice
#
#  (c) 2008-2011 Chi Hoang 
#  http://www.chihoang.de
#
##################################################################
#

echo "1" > /dev/cgroup/cpu/desktop/notify_on_release 
echo "1" > /dev/cgroup/cpu/applications/notify_on_release 
echo "555555" > /dev/cgroup/cpu/cpu.rt_runtime_us
echo "600000" > /dev/cgroup/cpu/cpu.rt_period_us
echo "555555" > /dev/cgroup/cpu/user/cpu.rt_runtime_us
echo "600000" > /dev/cgroup/cpu/user/cpu.rt_period_us
echo "1024" > /dev/cgroup/cpu/cpu.shares
echo "1024" > /dev/cgroup/cpu/desktop/cpu.shares
echo "1024" > /dev/cgroup/cpu/applications/cpu.shares
echo "25M" > /dev/cgroup/cpu/desktop/memory.limit_in_bytes
echo "95M" > /dev/cgroup/cpu/user/memory.limit_in_bytes

gconftool -s -t int /apps/hildon/update-notifier/check_interval 1440000

/usr/bin/schedtool -F -p 99 `pidof Xorg`
/usr/bin/schedtool -I `pidof hildon-desktop`
/usr/bin/schedtool -D `pidof trackerd`
/usr/bin/schedtool -D `pidof tracker-indexer`
/usr/bin/schedtool -I `pidof mafw-dbus-wrapper`
/usr/bin/schedtool -I `pidof pulseaudio`

drivelist=$(sfdisk -lnd /dev/mmcblk1)
swapmicro=$(echo "$drivelist" -n | grep Id=82 | awk '/mmcblk1/ {print $1}')
if [ "$swapmicro" ]; then
    swapon $swapmicro
    swapoff /dev/mmcblk0p3
    swapon /dev/mmcblk0p3
    /usr/bin/read-ahead $swapmicro 512
fi

#cat /proc/sys/vm/oom_kill_allocating_task
#cat /proc/sys/vm/laptop_mode
#cat /proc/sys/vm/page-cluster
#cat /proc/sys/vm/dirty_expire_centisecs
#cat /proc/sys/vm/dirty_writeback_centisecs
#cat /proc/sys/vm/dirty_background_ratio
#cat /proc/sys/vm/dirty_ratio
#0,0,5,500,500,10,40

# Speed Einstellungen
# Bei Wieviel Prozent freien Speicher soll das System anfange zu swappen
echo 45 > /proc/sys/vm/swappiness
#Wie oft soll der Kernel pruefen ob "dirty changes" vorhanden sind
#um diese dann auf die Platte zu schreiben (Zentisekunden)
echo 1250 > /proc/sys/vm/dirty_writeback_centisecs
#Wie alt muessen "dirty changes" sein damit sie weg geschrieben werden
#Sinnvollerweise vm.dirty_expire_centisecs = vm.dirty_writeback_centisecs
echo 1250 > /proc/sys/vm/dirty_expire_centisecs
#Wie viel Prozent das RAMS duerfen von einem Prozess mir "dirty changes" gefuellt
#sein bevor der Prozess gezwungen wird diese Aenderungen zu schreiben 40:10
echo 25 > /proc/sys/vm/dirty_ratio
#echo 10 > /proc/sys/vm/dirty_background_ratio
#Diese aktiviert oder deaktiviert die Toetung der OOM-Triggerung Aufgabe in
#Out-of-Memory-Situationen. Verbessert die Stabilität.
echo 1 > /proc/sys/vm/oom_kill_allocating_task
#vm.vfs_cache_pressure (0-250, 100 ist Standard) welche festlegt ob und wie
#sehr Prozesse ausgeswappt werden um Arbeitsspeicher fuer Cache frei zu machen.
echo 250 > /proc/sys/vm/vfs_cache_pressure
#default 0
echo 5 > /proc/sys/vm/laptop_mode
#default 22
echo 30 > /proc/sys/vm/lowmem_reserve_ratio
#vm.overcommit_memory (0=default, 1=malloc always succeeds(?!?), 2=strict overcommit)
#vm.overcommit_ratio (50=default, I used 100)
echo 2 > /proc/sys/vm/overcommit_memory
echo 100 > /proc/sys/vm/overcommit_ratio
#default 5
echo 7 > /proc/sys/vm/page-cluster
#echo 4096 > /proc/sys/vm/min_free_kbytes
echo 512 > /proc/sys/vm/min_free_kbytes

echo ondemand > /sys/devices/system/cpu/cpu0/cpufreq/scaling_governor
echo 15 > /sys/devices/system/cpu/cpu0/cpufreq/ondemand/up_threshold
echo 0 > /sys/devices/system/cpu/cpu0/cpufreq/ondemand/ignore_nice_load
#echo 4000000 >/sys/devices/system/cpu/cpu0/cpufreq/ondemand/sampling_rate
echo 0 >/sys/devices/system/cpu/cpu0/cpufreq/ondemand/powersave_bias
#echo 30 > /sys/devices/system/cpu/cpu0/cpufreq/conservative/freq_step
#echo 40 > /sys/devices/system/cpu/cpu0/cpufreq/conservative/down_threshold
#chmod 444 /sys/devices/system/cpu/cpu0/cpufreq/scaling_min_freq

echo 1 > /proc/sys/kernel/sched_compat_yield

#default 128
echo 16 > /sys/block/mmcblk0/queue/nr_requests
echo 16 > /sys/block/mmcblk1/queue/nr_requests

# default 256 
/usr/bin/read-ahead /dev/mmcblk0p1 512
/usr/bin/read-ahead /dev/mmcblk0p2 512
/usr/bin/read-ahead /dev/mmcblk0p3 512

echo 0 > /sys/block/mmcblk0/queue/iosched/slice_idle
echo 0 > /sys/block/mmcblk1/queue/iosched/slice_idle
echo 32 > /sys/block/mmcblk0/queue/iosched/quantum
echo 32 > /sys/block/mmcblk1/queue/iosched/quantum
echo 80 > /sys/block/mmcblk0/queue/iosched/fifo_expire_sync
echo 80 > /sys/block/mmcblk1/queue/iosched/fifo_expire_sync
echo 180 > /sys/block/mmcblk0/queue/iosched/fifo_expire_async
echo 180 > /sys/block/mmcblk1/queue/iosched/fifo_expire_async

# default 256
echo 400 > /proc/sys/kernel/threads-max

#
# Network Subsystem Options
#
# Renew ip_dynaddr at each dialin (default 0!)
echo 1 > /proc/sys/net/ipv4/ip_dynaddr
#
# Network Subsystem Time Management
#
# turns TCP timestamp support off, default 1, reduces CPU use
echo 0 > /proc/sys/net/ipv4/tcp_timestamps
#
# Wenn bei einer TCP-Verbindung die Gegenseite dem Wunsch zum Abbau der
# Verbindung nicht nachkommt, wird diese nach Ablauf dieser Zeitspanne
# (Sekunden) gekappt.
echo 5 > /proc/sys/net/ipv4/tcp_fin_timeout
#
# Zeit in Sekunden, nach der ein ungenutztes Fragment eines IP-Paketes
# verworfen wird (falls nicht alle Fragmente ihr Ziel erreichen, wird
# sichergestellt, dass unvollständige Pakete nicht ewig im Speicher
# verharren)
echo 5 > /proc/sys/net/ipv4/ipfrag_time
#
# Increase the tcp-time-wait buckets pool size
# default: 180000
tcp_max=180000
let tcp_max=400*$tcp_max/100
echo $tcp_max > /proc/sys/net/ipv4/tcp_max_tw_buckets
#
# Zeit des regelmäßigen Aussendens von Testpaketen, um eine Verbindung
# offen zu halten. Die Uhr startet neu, sobald ein reguläres Paket
# über die Verbindung geht
echo 300 > /proc/sys/net/ipv4/tcp_keepalive_time
#
# Maximale Zeit, die eine unbenutzte Route im Cache verweilen kann
echo 5 > /proc/sys/net/ipv4/route/mtu_expires
#
#
# Network Subsystem Packet Management
#
# Stop Source-Routing
for i in /proc/sys/net/ipv4/conf/*; do echo 0 > $i/accept_source_route 2> /dev/null; done
#
# Stop Redirecting
for i in /proc/sys/net/ipv4/conf/*; do echo 0 > $i/accept_redirects 2> /dev/null; done
#
# Reverse-Path-Filter
for i in /proc/sys/net/ipv4/conf/*; do echo 2 > $i/rp_filter 2> /dev/null; done
#
# BOOTP-Relaying ausschalten
for i in /proc/sys/net/ipv4/conf/*; do echo 0 > $i/bootp_relay 2> /dev/null; done
#
# Proxy-ARP ausschalten
for i in /proc/sys/net/ipv4/conf/*; do echo 0 > $i/proxy_arp 2> /dev/null; done
#
# Stop secure_redirects
for i in /proc/sys/net/ipv4/conf/*; do echo 0 > $i/secure_redirects 2> /dev/null; done
#
# Stop send_redirects
for i in /proc/sys/net/ipv4/conf/*; do echo 0 > $i/send_redirects 2> /dev/null; done
#
# router
echo 1 > /proc/sys/net/ipv4/ip_forward
#
# Anzahl Testpakete, die ausgesendet werden, um festzustellen,
# ob der Partner aktiv ist. Danach gilt die Verbindung als unterbrochen
echo 3 > /proc/sys/net/ipv4/tcp_keepalive_probes
#
# whatever
echo 1 > /proc/sys/net/ipv4/tcp_tw_recycle
#
# unterbreche Verbindungsaufbau nach 3 SYN-Paketen
# Default ist 6
echo 2 > /proc/sys/net/ipv4/tcp_syn_retries
#
# unterbreche Verbindungsaufbau nach 3 SYN/ACK-Paketen
# Default ist 6
echo 2 > /proc/sys/net/ipv4/tcp_synack_retries
#
# Anzahl Versuche, eine ICMP-Nachricht zuzustellen
echo 3 > /proc/sys/net/ipv4/route/redirect_number
#
# default:512
echo 1000 > /proc/sys/net/ipv4/route/gc_thresh
#
# Adjust to arp table gc to clean-up more often
# default:60
for i in /proc/sys/net/ipv4/neigh/*; do echo 30 > $i/gc_stale_time 2> /dev/null; done
#
# Increase TCP
# default:64
for i in /proc/sys/net/ipv4/neigh/*; do echo 96 > $i/proxy_qlen 2> /dev/null; done
#
# default:3
for i in /proc/sys/net/ipv4/neigh/*; do echo 6 > $i/unres_qlen 2> /dev/null; done
#
#
# Network Subsystem Memory Management
#
# Enable really big (>65kB) TCP window scaling if we want it.
echo 1 > /proc/sys/net/ipv4/tcp_window_scaling
#
# Turn on the tcp_sack
echo 1 > /proc/sys/net/ipv4/tcp_sack
#
# tcp_fack should be on because of sack
echo 1 > /proc/sys/net/ipv4/tcp_fack
#
# Sometimes, packet reordering in a network can be interpreted as packet loss and 
# hence increasing the value of this parameter should improve performance (default is “3″
# Set TCP Re-Ordering value in kernel
# default:3
echo 48 > /proc/sys/net/ipv4/tcp_reordering
#
# Enables/Disables the behaviour of cachek performance charecteristics connection. 
# By default, Linux Kernel remembers connection performance and congestion charecteristics.
# default = 0
echo 1 > /proc/sys/net/ipv4/tcp_no_metrics_save
#
# You can set this to one of the manu available high speed congestion variants like “cubic” 
# “hs-tcp” (default is “reno”)
# default = "reno"
#echo "cubic" > /proc/sys/net/tcp_congestion_control
#
# ipfrag_high_thresh setzt das Limit für den maximalen Speicher,
# der für das fragmentieren von Paketen reserviert wird. Wenn mehr Speicher
# benötigt wird fängt der Kernel an fragmentierte Pakete zu
# verwerfen. Dieses Limit ist sehr kritisch für Denial of Service Angriffe,
# da es, wenn es zu niedrig gewählt wird zu viele Pakete verwirft, und wenn
# es zu hoch gewählt wird zu viel Speicher, und CPU Leistung verbrauchen
# kann. ipfrag_low_thresh ist für das minimale Limit zuständig.
#
# default high: 262144 (256kb)
# default low:  196608 (192kb)
#
# einfach mal 300% mehr nehmen
high=262144
low=196608
let high=400*$high/100
let low=400*$low/100
echo $high > /proc/sys/net/ipv4/ipfrag_high_thresh
echo $low > /proc/sys/net/ipv4/ipfrag_low_thresh
#
# Länge der Warteschlange für eingehende TCP-Verbindungsanforderungen für
# einen Socket (default 300)
echo 3000 > /proc/sys/net/ipv4/tcp_max_syn_backlog
#
# add more conntrack
# default 16384
#echo 25000 > /proc/sys/net/nf_conntrack_max
#
# Länge der Warteschlange der routing tabelle (experimental)
# default:8192 (gc_thresh x 16)
echo 10000 > /proc/sys/net/ipv4/route/max_size
#
# TCP Autotuning setting. "The tcp_mem variable defines how the TCP stack should behave when it comes to memory usage. ...
# The first value specified in the tcp_mem variable tells the kernel the low threshold. Below this point, the TCP stack do
# not bother at all about putting any pressure on the memory usage by different TCP sockets. ... The second value tells the
# kernel at which point to start pressuring memory usage down. ... The final value tells the kernel how many memory pages it
# may use maximally. If this value is reached, TCP streams and packets start getting dropped until we reach a lower memory
# usage again. This value includes all TCP sockets currently in use."
#
# min:      65535  (64kb)
# default: 131071 (128kb)
# test:    262144 (256kb)

tmem=262144
let tmem_max=400*$tmem/100
echo 4096 $tmem $tmem_max > /proc/sys/net/ipv4/tcp_mem

# TCP Autotuning setting. "The first value tells the kernel the minimum receive buffer for each TCP connection, and this
# buffer is always allocated to a TCP socket, even under high pressure on the system. ... The second value specified tells
# the kernel the default receive buffer allocated for each TCP socket. This value overrides the /proc/sys/net/core/rmem_default
# value used by other protocols. ... The third and last value specified in this variable specifies the maximum receive buffer
# that can be allocated for a TCP socket."
#
# Increase the maximum and default receive socket buffer size
# (Increase default size for 4 Mbytes/s)
# default: 131071 (128kb)
# test:    262144 (256kb)
#
rmem=262144
let rmem_max=400*$rmem/100
echo $rmem > /proc/sys/net/core/rmem_default
echo $rmem_max > /proc/sys/net/core/rmem_max
echo 4096 $rmem $rmem_max > /proc/sys/net/ipv4/tcp_rmem
#
# TCP Autotuning setting. "This variable takes 3 different values which holds information on how much TCP sendbuffer memory
# space each TCP socket has to use. Every TCP socket has this much buffer space to use before the buffer is filled up. Each of
# the three values are used under different conditions. ... The first value in this variable tells the minimum TCP send buffer
# space available for a single TCP socket. ... The second value in the variable tells us the default buffer space allowed for a
# single TCP socket to use. ... The third value tells the kernel the maximum TCP send buffer space."
# min:      65535  (64kb)
# default: 131071 (128kb)
# test:    262144 (256kb)
#
wmem=262144
let wmem_max=400*$wmem/100
echo $wmem > /proc/sys/net/core/wmem_default
echo $wmem_max > /proc/sys/net/core/wmem_max
echo 4096 $wmem $wmem_max > /proc/sys/net/ipv4/tcp_wmem
#
# Maximum ancillary buffer size allowed per socket. Ancillary data is a 
# sequence of struct cmsghdr structures with appended data. 
# The default size is 10240 bytes
optmem_max=10240
let max=400*$optmem_max/100
echo $max > /proc/sys/net/core/optmem_max
#
# default: 10
echo 64 > /proc/sys/net/unix/max_dgram_qlen
#
# default: 50
echo 128 > /proc/sys/net/core/message_burst
#
# default: 290
#echo 512 > /proc/sys/net/core/mod_cong
#
# default: 100
#echo 128 > /proc/sys/net/core/lo_cong
#
# default: 20
#echo 64 > /proc/sys/net/core/no_cong
#
# default: 20
#echo 64 > /proc/sys/net/core/no_cong_thresh
#
#
# Listed below are general net settings, like "netdev_max_backlog" (typically 300), the length of all your
# network packets. This value can limit your network bandwidth when receiving packets, Linux has to wait
# up to scheduling time to flush buffers (due to bottom half mechanism), about 1000/HZ ms
#
#   300    *        100             =     30 000
# packets     HZ(Timeslice freq)         packets/s
#
# 30 000   *       1000             =      30 M
# packets     average (Bytes/packet)   throughput Bytes/s
#
# If you want to get higher throughput, you need to increase netdev_max_backlog, by typing:
# echo 4000 > /proc/sys/net/core/netdev_max_backlog
#
# default value: 300
echo 3048 > /proc/sys/net/core/netdev_max_backlog
#
# Set timeout on kernel panics (auto reboots after # seconds):
# default:0 (no autoreboot)
#
#echo 60 > /proc/sys/kernel/panic
#
# less shmmax
# default: 33554432/1024/1024 = 32 MB,
# this machine has only 16 MB so what the fuck is this!!!
# recommendation is 50% of machine memory
shmmax=268435456
let shmmax=25*$shmmax/100
#echo $shmmax > /proc/sys/kernel/shmmax
#
# PAGE_SIZE: now 4096 (see patches-Archives) (default 16384)
# wenn Bytes, dann gleich SHMMAX; wenn Seiten, dann ceil(SHMMAX/PAGE_SIZE)
# ich nehme mal ceil(SHMMAX/PAGE_SIZE)
# default: 2097152
# mplayer bug shmall must be greater 32678
#echo 32678 > /proc/sys/kernel/shmall
#
# default: 4096
#echo 128 > /proc/sys/kernel/shmmni
#
#
# Kernel-Message-Queue
#
# default: 8192
#echo 2048 > /proc/sys/kernel/msgmax
#
# default: 16384
#echo 2048 > /proc/sys/kernel/msgmnb
#
#
# This will enusre that immediatly subsequent connections use these values.
echo 1 > /proc/sys/net/ipv4/route/flush
#
#
#
